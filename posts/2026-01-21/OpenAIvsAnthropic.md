---

title: "OpenAI, Anthropic Score Low on Human-Level AI Safety Index" 
category: "AI SAFETY" 
author: "Siméon Campos" 
readTime: "5 min read"

---

A scathing report by the Future of Life Institute (FLI) has awarded leading AI firms OpenAI and Anthropic low marks for their preparedness for risks associated with human-level intelligence. The index, which evaluated seven major developers across categories like "existential safety" and "current harms," found that safety frameworks lack the rigor of established engineering fields like aerospace or nuclear power. While Anthropic received the highest grade of C+, OpenAI followed with a C, and Google DeepMind trailed with a C-. The report highlights that as AI systems become more autonomous and capable of modifying their own source code, the "black box" nature of these models creates verification challenges that current safety protocols are unable to address. Scientists warn that without a fundamental shift toward specification and robustness, the risks of models escaping human control remain unacceptably high.

The safety debate is not just a theoretical concern for researchers; it is a pragmatic barrier to enterprise adoption. Gartner’s analysis suggests that 2026 will be the "Trough of Disillusionment" for AI, as businesses struggle to see a clear return on investment.5 To counter this, companies like OpenAI and Anthropic have released studies claiming that workers save up to an hour a day using their tools.14 However, these internal surveys often fail to account for the quality of the output or the "AI workslop" that must be cleaned up, which reportedly costs some businesses hundreds of hours a week.15
